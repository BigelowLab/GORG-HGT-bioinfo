{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8888a8ca-15f3-4856-9d59-ba4af3cdf6e8",
   "metadata": {},
   "source": [
    "## Comparing shared ORFs to ANI per genome\n",
    "\n",
    "---  \n",
    "#### Calculating the number of orthologous genes per genome pair at different pct_id thresholds, and merging this information with calculated whole genome ANI values per pair.\n",
    "\n",
    "#### High level steps:\n",
    "1. Run pyANI comparison of all genomes\n",
    "1. Call genes on genomes using prokka (I used standard SCGC prokka outputs in the original analysis)\n",
    "    a. make sure that gene names follow a pattern in which the first identifier per gene name identifies the genome the gene came from, followed by an underscore (e.g. AG-910-A01_gene1)  \n",
    "    b. copy appropriately formatted \\*.fnn files to a working data directory (e.g. data/fnn)\n",
    "2. Create a BLAST database of all ORFs in genome collection\n",
    "3. Run BLASTn of each individual ffn against the database of all ORFs\n",
    "4. Summarize each BLAST file per genome pair, write each summary per SAG to a file\n",
    "5. Combine all summaries into one file, combine with pyANI output\n",
    "\n",
    "\n",
    "--- \n",
    "### Specific instructions\n",
    "\n",
    "#### Compare Genomes via pyANI\n",
    "\n",
    "e.g.  \n",
    "\n",
    "```\n",
    "module load anaconda3/2019.07\n",
    "\n",
    "indir=data/contigs/\n",
    "outdir=analyses/pyani/\n",
    "\n",
    "average_nucleotide_identity.py -o $outdir \\\n",
    "-i ${indir} -m ANIb --workers 20\n",
    "```\n",
    "\n",
    "#### BLASTn Open Reading Frame Comparison\n",
    "See the following cells for code for the BLASTn analysis\n",
    "\n",
    "1. Begin with called ORF na sequences from each genome, placed in the same directory.  \n",
    "    a. I copied all \\*.fnn files from each SAG in the collection into the directory data/ffn/\n",
    "2. Create BLAST database of all ORFs from all genomes that you want to compare with below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe092b2b-333d-44b9-b5e9-a22a7cb9b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os.path as op\n",
    "import shutil\n",
    "import itertools\n",
    "\n",
    "def readfa(fh):\n",
    "    for header, group in itertools.groupby(fh, lambda line: line[0] == '>'):\n",
    "        if header:\n",
    "            line = next(group)\n",
    "            name = line[1:].strip()\n",
    "        else:\n",
    "            seq = ''.join(line.strip() for line in group)\n",
    "            yield name, seq\n",
    "            \n",
    "def write_fa_record(name, seq, oh, line_len=60):\n",
    "    print(\">{}\".format(name), file=oh)\n",
    "    for i in range(0, len(seq), line_len):\n",
    "        print(seq[i:i+line_len], file=oh)\n",
    "\n",
    "def get_sag_orfs(sag, prokka_dir):\n",
    "    ffn = op.join(prokka_dir, \"{}.ffn\".format(sag, sag))\n",
    "    if op.exists(ffn):\n",
    "        return ffn\n",
    "    else:\n",
    "        print(\"could not find ffn for {}\".format(sag))\n",
    "        return None\n",
    "\n",
    "def safe_makedir(dname):\n",
    "    \"\"\"\n",
    "    Make a directory if it doesn't exist, handling concurrent race conditions.\n",
    "    \"\"\"\n",
    "    if not dname:\n",
    "        return dname\n",
    "    num_tries = 0\n",
    "    max_tries = 5\n",
    "    while not os.path.exists(dname):\n",
    "        try:\n",
    "            os.makedirs(dname)\n",
    "        except OSError:\n",
    "            if num_tries > max_tries:\n",
    "                raise\n",
    "            num_tries += 1\n",
    "            time.sleep(2)\n",
    "    return dname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f05bfd-d479-440e-be33-1352fea2dc47",
   "metadata": {},
   "source": [
    "define where you want to write the ffn and blast database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295b4255-1ffe-4ba7-84f1-b2a3e189a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn_dir = safe_makedir(\"data/ffn/\")\n",
    "database = safe_makedir(\"data/blast_database/\")\n",
    "db_fasta = op.join(database, \"combined_ffas.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76636d8-6792-433b-92f9-5be000682256",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffns = glob.glob(ffn_dir, \"*.ffn\")\n",
    "\n",
    "with open(db_fasta, 'w') as oh:\n",
    "    for ffn in ffns:\n",
    "        for name, seq in readfa(open(ffn)):\n",
    "            write_fa_record(name, seq, oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f0b9c5-bd90-4ed3-9973-4f769f3cea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!makeblastdb -in {db_fasta} -dbtype nucl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef189583-2ef8-4312-b36f-3efaf8b7587c",
   "metadata": {},
   "source": [
    "#### BLAST each genome individually against the database of all genomes.  \n",
    "    a. see blastn.snakemake, pasted below, edit as appropriate  \n",
    "    b. pay attention to the max-target-seqs parameter of blast... this may be important if you have a lot of similar genomes in your collection (as we did in GORG-Tropics)...  \n",
    "    \n",
    "blastn.snakemake:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f42544-243a-4a61-ac87-ad20fc62c879",
   "metadata": {},
   "source": [
    "```\n",
    "import os.path as op\n",
    "import os\n",
    "\n",
    "### EDIT ME ###\n",
    "\n",
    "# path to directory with all ffn files:\n",
    "ffn_dir = \"/mnt/scgc/scgc_nfs/lab/julia/notebooks/panspecies/fig3/data/ffn/\"\n",
    "\n",
    "# path to BLAST database\n",
    "database = \"/mnt/scgc/scgc_nfs/lab/julia/notebooks/panspecies/fig3/data/blast_database/gorg-tropics_16s_80pct_complete_orfs.fasta\"\n",
    "\n",
    "# path to directory to write all BLAST outputs to\n",
    "outdir = \"/mnt/scgc/scgc_nfs/lab/julia/notebooks/panspecies/fig3/analyses/blastn/\"\n",
    "\n",
    "SAMPLES, = glob_wildcards(op.join(ffn_dir,'{sample}.ffn'))\n",
    "\n",
    "# path to write snakemake logs to\n",
    "logdir = '/home/julia/out/210818_snakemake'\n",
    "\n",
    "### \n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        expand(op.join(outdir, '{sample}_vs_gt-16s-80comp-orfs.out'), sample=SAMPLES)\n",
    "\n",
    "rule blastn:\n",
    "    input:\n",
    "        ffn = op.join(ffn_dir, '{sample}.ffn')\n",
    "    output:\n",
    "        outblast = op.join(outdir, '{sample}_vs_gt-16s-80comp-orfs.out')\n",
    "    params:\n",
    "        db = database,\n",
    "        threads = 8,\n",
    "        max_seqs = 2000\n",
    "    resources:\n",
    "        queue = 'route',\n",
    "        cpus = '10',\n",
    "        walltime = '5:00:00',\n",
    "        mem = '10G',\n",
    "        log = logdir\n",
    "    shell:\n",
    "        \"module purge; module load anaconda3; \\\n",
    "        blastn -db {params.db} \\\n",
    "        -query {input.ffn} \\\n",
    "        -outfmt '6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qlen slen' \\\n",
    "        -num_threads {params.threads} -max_target_seqs {params.max_seqs} \\\n",
    "        -out {output.outblast}\"\n",
    "        \n",
    "\n",
    "# How to submit to scheduler using this snakemake file:\n",
    "'''snakemake -s /mnt/scgc/scgc_nfs/lab/julia/notebooks/panspecies/fig3/subs/blastn.snakemake --cluster \\\n",
    "\"qsub -q {resources.queue} -j oe -o {resources.log} -l ncpus={resources.cpus},mem={resources.mem},walltime={resources.walltime}\" \\\n",
    "-j 900 -k --latency-wait 20'''\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8438ad93-4138-4926-8631-2e69b5febc43",
   "metadata": {},
   "source": [
    "Process and summarize BLAST outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6daee93-4a94-48b0-a8cd-04938f6e4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import os.path as op\n",
    "import glob\n",
    "import glob\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# You will need a table summarizing gene counts per SAG\n",
    "\n",
    "def create_orf_summary_table(ffn_list, outfile, length_threshold = [500, 1500]):\n",
    "    with open(outfile, 'w') as oh:\n",
    "        print('sag','gene_count','gene_count_{}-{}bp'.format(len_threshold[0], len_threshold[1]), sep = ',', file = oh)\n",
    "        for ffn in ffn_list:\n",
    "            sag = op.basename(ffn).split(\".\")[0]\n",
    "            gene_count = 0\n",
    "            thresh_count = 0\n",
    "            for name, seq in readfa(open(ffn)):\n",
    "                gene_count += 1\n",
    "                gene_length = len(seq)\n",
    "                if len_threshold[0] <= gene_length and len_threshold[1] >= gene_length:\n",
    "                    thresh_count += 1\n",
    "            print(sag, gene_count, thresh_count, sep = ',', file = oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e638149-79a8-49a5-a0ec-cfbc4805549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_blast_outputs = \"analyses/blastn/\"\n",
    "ffn_dir = \"data/ffn/\"\n",
    "\n",
    "# new files and directories\n",
    "table_dir = safe_makedir(\"tbls\")\n",
    "orf_summary_table = op.join(table_dir, \"sag_orf_summary.csv\")\n",
    "\n",
    "# list of all blast outputs\n",
    "blasts = glob.glob(op.join(path_to_blast_outputs, \"*.out\"))\n",
    "\n",
    "# list of all ffns\n",
    "ffns = glob.glob(op.join(ffn_dir, \"*.ffn\"))\n",
    "\n",
    "create_orf_summary_table(ffns, orf_summary_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d5ac2d-7431-4781-b31a-6e5f95a5a9f3",
   "metadata": {},
   "source": [
    "### Process each of the blast files, write each to a new summary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c66d0-a7b9-4b4f-a0e9-9fadc015195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to process blast files:\n",
    "\n",
    "def process_blast(blastin, \n",
    "                  orf_count_df,\n",
    "                  pid_list = [99.9, 99.8, 99.7, 99.6, 99.5, 99, 98.5, 98, 97.5, 97], \n",
    "                  len_threshold = [500, 1500]):\n",
    "    names = 'qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qlen slen'.split()\n",
    "    df = pd.read_csv(blastin, sep = \"\\t\", names = names)\n",
    "\n",
    "    df['qsag'] = [i.split(\"_\")[0] for i in df['qseqid']]              \n",
    "    df['ssag'] = [i.split(\"_\")[0] for i in df['sseqid']]\n",
    "    df = df[df['qsag'] != df['ssag']]\n",
    "    df = df[(df['length'] > (0.8 * df['qlen'])) | (df['length'] > 0.8 * df['slen'])]\n",
    "    \n",
    "    newdf = pd.DataFrame(columns = ['qsag','ssag'])\n",
    "    \n",
    "    ni_alpha_mean = df.groupby(['qsag','ssag'], as_index = False)['pident'].mean().rename(columns = {'pident':'mean_pident'}).merge(\n",
    "                    df.groupby(['qsag','ssag'])['pident'].std().reset_index().rename(columns = {'pident':'stdev_pident'})).merge(\n",
    "                    df.groupby(['qsag','ssag'], as_index = False)['pident'].count().rename(columns = {'pident':'total_hits'})).merge(\n",
    "                    df.groupby(['qsag','ssag'], as_index = False)['pident'].median().rename(columns = {'pident':'median_pident'}))\n",
    "    \n",
    "    newdf = newdf.merge(ni_alpha_mean, how = 'outer').merge(\n",
    "            orf_count_df.rename(columns = {i:'s{}'.format(i) for i in orf_count_df.columns}), how = 'left').merge(\n",
    "            orf_count_df.rename(columns = {i:'q{}'.format(i) for i in orf_count_df.columns}), how = 'left')\n",
    "    \n",
    "    for pid in pid_list:\n",
    "        \n",
    "        len_bound = df[(df['pident'] >= pid) & \n",
    "                       ((df['qlen'] >= len_threshold[0]) & (df['qlen'] <= len_threshold[1])) & \n",
    "                      ((df['slen'] >= len_threshold[0]) & (df['slen'] <= len_threshold[1]))]\\\n",
    "                        .groupby(['qsag','ssag'], as_index=False)['sseqid']\\\n",
    "                        .count().rename(columns = {'sseqid':'{}_pid_{}-{}bp_orthologs'.format(pid, len_threshold[0],len_threshold[1])})\n",
    "        all_sizes = df[df['pident'] >= pid].groupby(['qsag','ssag'], as_index=False)['sseqid']\\\n",
    "                        .count().rename(columns = {'sseqid':'{}_pid_orthologs'.format(pid, len_threshold[0],len_threshold[1])})\n",
    "        newdf = newdf.merge(\n",
    "                len_bound, how = 'outer').merge(\n",
    "                all_sizes, how = 'outer')\n",
    "        \n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd577d-4abe-4537-9f8c-3b2217d92dcb",
   "metadata": {},
   "source": [
    "Apply function to all blast files, write each to an summary file per blast file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd78324-8b32-463a-b606-9d97819b681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_summary_dir = safe_makedir(\"analyses/blastn_summaries\")\n",
    "\n",
    "orf_count_df = pd.read_csv(orf_summary_table)\n",
    "\n",
    "blasts = glob.glob(op.join(path_to_blast_outputs,\"*\")\n",
    "\n",
    "for blast in blasts:\n",
    "    out = op.join(blast_summary_dir, '{}_summary.csv'.format(op.basename(blast).split(\"_\")[0]))\n",
    "    process_blast(blast, orf_count_df).sort_values(by = 'total_hits', ascending=False).to_csv(out, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b66a78-6673-4e3a-9355-d5420466a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all results into a large dataframe:\n",
    "\n",
    "giantdf = pd.concat([pd.read_csv(i) for i in glob.glob('analyses/blast_summaries/*')])\n",
    "\n",
    "# add a pairs column to have a unique identifier per genome pair:\n",
    "pairs = []\n",
    "\n",
    "for i, l in giantdf.iterrows():\n",
    "\n",
    "    lst = [l['qsag'], l['ssag']]\n",
    "    lst.sort()\n",
    "    pairs.append(\"_\".join(lst))\n",
    "\n",
    "giantdf['pair'] = pairs\n",
    "\n",
    "ortho_names = [i for i in giantdf.columns if 'orthologs' in i]\n",
    "\n",
    "# dropping redundant counts, as this was essentially a reciprocal BLAST... have confirmed this in other analyses\n",
    "sdf = giantdf.drop_duplicates(subset = 'pair')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a13a25-a23d-4a8c-b7c1-16fbc71d563b",
   "metadata": {},
   "source": [
    "#### Load pyani results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa89cdd8-828f-4682-945e-959b25309c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pyani_results(pyani_outdir, method='ANIb', min_coverage = 0.03, scgc=True):\n",
    "    identity = op.join(pyani_outdir,'{}_percentage_identity.tab'.format(method))\n",
    "    lengths = op.join(pyani_outdir, '{}_alignment_lengths.tab'.format(method))\n",
    "    coverage = op.join(pyani_outdir, '{}_alignment_coverage.tab'.format(method))\n",
    "    simerrors = op.join(pyani_outdir, '{}_similarity_errors.tab'.format(method))\n",
    "    \n",
    "    load_matrix_table = lambda metric, table: pd.read_csv(table, sep=\"\\t\").melt(id_vars='Unnamed: 0', var_name='Genome B', value_name=metric).rename(columns={'Unnamed: 0':'Genome A'})\n",
    "    \n",
    "    idf = load_matrix_table('ani', identity)\n",
    "    ldf = load_matrix_table('ani_aln_len', lengths)\n",
    "    cdf = load_matrix_table('ani_aln_cov', coverage)\n",
    "    edf = load_matrix_table('ani_similarity_errs', simerrors)\n",
    "    \n",
    "    bdf = idf.merge(ldf).merge(cdf).merge(edf)\n",
    "    bdf = bdf[bdf['Genome A'] != bdf['Genome B']]\n",
    "    \n",
    "    if scgc:\n",
    "        bdf['Genome A'] = [i.split(\"_\")[0] for i in bdf['Genome A']]\n",
    "        bdf['Genome B'] = [i.split(\"_\")[0] for i in bdf['Genome B']]\n",
    "    \n",
    "    comps = []\n",
    "\n",
    "    for i, l in bdf.iterrows():\n",
    "        lst = [l['Genome A'], l['Genome B']]\n",
    "        lst.sort()\n",
    "        string = \"{}_{}\".format(lst[0], lst[1])\n",
    "        comps.append(string)\n",
    "    bdf['comp'] = comps\n",
    "    bdf = bdf.sort_values(by=['comp','ani'], ascending=False)\n",
    "    \n",
    "    dfa = bdf.drop_duplicates(subset=['comp'], keep='first')\n",
    "    dfb = bdf.drop_duplicates(subset=['comp'], keep='last')\n",
    "    dfa.rename(columns={'ani_aln_cov':'ani_aln_cov_ab', 'ani_aln_len':'ani_aln_len_ab', \n",
    "                       'ani_similarity_errs':'ani_similarity_errs_ab'}, inplace=True)\n",
    "    dfb.rename(columns={'ani_aln_cov':'ani_aln_cov_ba', 'ani_aln_len':'ani_aln_len_ba', \n",
    "                       'ani_similarity_errs':'ani_similarity_errs_ba'}, inplace=True)\n",
    "\n",
    "    dfab = dfa.merge(dfb[['comp', 'ani_aln_cov_ba','ani_aln_len_ba','ani_similarity_errs_ba']], on='comp', how='outer')\n",
    "    outdf = dfab[(dfab['ani_aln_cov_ba'] > min_coverage) & (dfab['ani_aln_cov_ab'] > min_coverage)]\n",
    "    return outdf\n",
    "\n",
    "# load the ani df created by pyani:\n",
    "\n",
    "pyanidir = \"analyses/pyaniout/\"\n",
    "\n",
    "anidf = load_pyani_results()\n",
    "pairs2  = []\n",
    "\n",
    "for i, l in anidf.iterrows():\n",
    "    lst = [l['Genome A'], l['Genome B']]\n",
    "    lst.sort()\n",
    "    pairs2.append(\"_\".join(lst))\n",
    "\n",
    "anidf = anidf.rename(columns = {'comp':'pair'})\n",
    "\n",
    "anidf = anidf[['Genome A','Genome B','ani', 'ani_aln_coverage_ab','ani_aln_coverage_ba', 'pair']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e16da-b28c-4b12-b55b-b1a4b7d0120a",
   "metadata": {},
   "source": [
    "Merge anidf and the dereplicated BLAST ortholog comparison table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a7fc62-0bee-4e8f-b023-a5fd21670086",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.merge(anidf, on = 'pair', how = 'outer')\n",
    "\n",
    "# add GND for Ramunas\n",
    "sdf['GND'] = [round(100 - i * 100, 2) for i in sdf['ani']]\n",
    "\n",
    "# include all pairs for which ANI was measured in summary calculations:\n",
    "sdf[ortho_names] = sdf[ortho_names].fillna(0)\n",
    "\n",
    "# create ani bins\n",
    "tups = [(round(i-0.01, 2), round(i, 2),) for i in np.arange(1, 0.6, -0.01)]\n",
    "bins = pd.IntervalIndex.from_tuples(tups)\n",
    "sdf['ani_bin'] = pd.cut(sdf['ani'], bins)\n",
    "\n",
    "# write to csv:\n",
    "sdf.to_csv(\"tbls/sag_pair_summary_shared_orfs_ani.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
